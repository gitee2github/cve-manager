#!/usr/bin/python3
import json
import re
import os
import pprint
from bs4 import BeautifulSoup
import requests
from .bugzilla import Api
from .settings import DEFAULT_SAVE_PATH
from .logger import logger, get_spider_looger_str
from .pipe import SavePipe, RequestsUrl


URL_REGEX = re.compile(
    r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
)
VERSION_REGEX = r"(\d+[.-]?)+([.-]?[A-Za-z0-9]*)*"

__all__ = ["Crawl", "Nvd", "Debian", "Ubuntu", "Bugzilla"]


class Crawl:
    """
    CVE platform base class
    """

    def __init__(self, cve, pkg, versions, out_path, **kwargs) -> None:
        self.cve = cve
        self.pkg = pkg
        self.versions = versions
        self.fix_versions = None
        self._folder = out_path
        self.__dict__.update(**kwargs)

    def parse(self, response, **kwargs):
        """
        All enabled entrances
        Args:
            response: response
            **kwargs: kwargs

        Returns:
            None
        """
        raise NotImplementedError(
            f"{self.__class__.__name__}.parse callback is not defined"
        )

    @staticmethod
    def _re_escape(name: str):
        """escape str

        Args:
            name (str): pkgname

        Returns:
            str: escape new pkg name
        """
        if "+" in name:
            name = name.replace("+", "\+")
        if "^" in name:
            name = name.replace("^", "\^")
        if "$" in name:
            name = name.replace("$", "\$")
        if "." in name:
            name = name.replace(".", "\.")
        if "|" in name:
            name = name.replace("|", "\|")
        if "-" in name:
            name = name.replace("-", "\-")
        return name

    @property
    def folder(self):
        """
        The CVE folder in the specified directory
        """
        folder = getattr(self, "_folder", DEFAULT_SAVE_PATH)
        if hasattr(self, "pkg") and hasattr(self, "cve"):
            folder = os.path.join(folder, self.pkg + "-" + self.cve)
        return folder

    def _create_folder(self, version, folder=None):
        """
        Create a folder Path
        Args:
            version: version
            folder: The folder defaults to None

        Returns:
            path: folder path
        """
        if not folder:
            folder = self.folder

        path = os.path.join(folder, str(version))
        os.makedirs(os.path.dirname(path), exist_ok=True)
        return path

    def create_folders(self, versions):
        """
        Creating Folder Collection
        Args:
            versions: Multiple versions

        Returns:
            folder: folder path
        """
        if not versions:
            return self.folder

        for version in versions:
            return self._create_folder(version)

    def git(self, func):
        """
        git
        """
        git_regex = "^http[s]?://git.({pkg_name}|.*).(org|net)(.*{pkg_name}([0-9]+)?(.git)?/commit.*|(/gitweb)?/\?p={pkg_name}([0-9]+)?.git;a=commit.*)".format(
            pkg_name=Crawl._re_escape(self.pkg)
        )

        def wrap(*args, **kwargs):
            url = kwargs.get("url", "")
            if re.match(git_regex, url, flags=re.IGNORECASE):
                url = re.sub("commit|commitdiff|commitdiff_plain", "patch", url)
                return url
            if re.match(
                "^https://git.*raw/([0-9a-z]*$|.*\.patch$)", url, flags=re.IGNORECASE
            ):
                return url

            return func(*args, **kwargs)

        return wrap

    def github(self, func):
        """
        github
        """
        github_regex = "^http[s]?://github.com.*{pkg_name}(.*)?/commit.*".format(
            pkg_name=Crawl._re_escape(self.pkg)
        )

        def wrap(*args, **kwargs):
            url = kwargs.get("url", "")
            if re.match(github_regex, url, flags=re.IGNORECASE):
                url = re.sub("\s+(.*)|\?.*", "", url)
                return url + ".patch"
            return func(*args, **kwargs)

        return wrap

    def gitlab(self, func):
        """
        gitlab
        """
        gitlab_regex = (
            "^http[s]?://gitlab(.*.org|.com).*{pkg_name}([0-9]+)?(/-)?/commit.*".format(
                pkg_name=Crawl._re_escape(self.pkg)
            )
        )

        def wrap(*args, **kwargs):
            url = kwargs.get("url", "")
            if re.match(gitlab_regex, url, flags=re.IGNORECASE):
                url = re.sub("\s+(.*)|\?.*", "", url)
                return url + ".patch"
            return func(*args, **kwargs)

        return wrap

    def match(self, url):
        """
        match url
        """

        @self.github
        @self.git
        @self.gitlab
        def _match_url(url):
            return None

        return _match_url(url=url)


class Versions:
    """
    Version number processing
    """

    separator = (".", "-")
    _connector = "&"

    def _order(self, version, separator=None):
        """
        Version of the cutting
        Args:
            version: version
            separator: separator

        Returns:

        """
        if not separator:
            separator = self._connector
        return tuple([int(v) for v in version.split(separator) if v.isdigit()])

    def _similar(self, text, compare_text):
        """
        Similarity comparison
        Args:
            text: text
            compare_text: compare_text

        Returns:

        """
        _text = text if len(text) < len(compare_text) else compare_text
        weight = 0
        for index, _ in enumerate(_text):
            if text[index] != compare_text[index]:
                weight = index
                break
        return weight / len(text) * 1.0 * 100

    def similarity(self, text, compare_queue):
        """
        Compare the similarity of two strings
        """
        if isinstance(compare_queue, str):
            return self._similar(text, compare_queue)
        ratio = [self._similar(text, compare_text) for compare_text in compare_queue]
        return ratio

    def match_version(self, pkg_name, fix_text):
        """
        Match the version number of the software from the repaired text
        """
        versions = list()
        for pkg_info in fix_text.split(","):
            if pkg_name.lower() in pkg_info.lower():
                _v = re.search(VERSION_REGEX, pkg_info)
                versions.append(_v.group())
        return versions

    def lgt(self, version, compare_version):
        """
        Returns true if the size of the compared version is greater
        than that of the compared version, or false otherwise

        """
        for separator in self.separator:
            version = self._connector.join([v for v in version.split(separator)])
            compare_version = self._connector.join(
                [v for v in compare_version.split(separator)]
            )
        version = self._order(version)
        compare_version = self._order(compare_version)
        return version >= compare_version


class Nvd(Crawl):
    """
    nvd plantform
    """

    nvd_host = "https://nvd.nist.gov"

    def __init__(self, cve, pkg, versions, out_path=DEFAULT_SAVE_PATH):
        super(Nvd, self).__init__(
            cve=cve, pkg=pkg, versions=versions, out_path=out_path
        )
        self.start_url = self.nvd_host + "/vuln/detail/" + cve

    def get_urls(self, resp_str, pkg):
        """
        Gets the URL of patch
        Args:
            resp_str: resp_str
            pkg: pkg

        Returns:

        """
        patch_urls = set()
        other_urls = set()
        pkg = Crawl._re_escape(pkg)

        soup = BeautifulSoup(resp_str, "html.parser")
        urls = [
            res.text
            for res in soup.find_all(
                name="td", attrs={"data-testid": re.compile("vuln-hyperlinks-link-\d+")}
            )
        ]
        for url in urls:
            seen_url = self.match(url=url)
            if seen_url:
                patch_urls.add(seen_url)
            else:
                other_urls.add(url)

        logger_str = get_spider_looger_str(
            "Nvd", self.pkg, self.versions, self.cve, patch_urls, other_urls
        )
        logger.info(logger_str)

        return patch_urls, other_urls

    def parse(self, response, **kwargs):
        """
        Parse response data
        Args:
            response: response
            **kwargs:

        Returns:

        """
        patch_urls, other_urls = self.get_urls(response.text, self.pkg)
        if not patch_urls:
            if not other_urls:
                text = "NVD result:There is no patch information and it has not been repaired"
                yield SavePipe(crawl=self, text=text)
            else:
                for other_url in other_urls:
                    text = "Uncertain patch information in Nvd：%s" % other_url
                    yield SavePipe(crawl=self, text=text)
        else:
            for url in patch_urls:
                text = "find patch information in Nvd：%s" % url
                yield SavePipe(crawl=self, text=text)
                yield RequestsUrl(url=url, crawl=self, is_patch=True)


class Debian(Crawl):
    """
    Get the PR address in the Debian website
    """

    def __init__(self, cve, versions, pkg, out_path=DEFAULT_SAVE_PATH):
        """
        Initialize attribute
        Args:
            cve_num: cve number
        """
        super(Debian, self).__init__(
            cve=cve, pkg=pkg, versions=versions, out_path=out_path
        )
        self.start_url = "https://security-tracker.debian.org/tracker/" + self.cve

    def parse(self, response, **kwargs):
        """
        Parse response data
        Args:
            response: response
            **kwargs:

        Returns:

        """

        patch_urls = []
        other_urls = []
        soup = BeautifulSoup(response.text, "html.parser")
        pre = soup.pre

        if pre:
            links = pre.find_all("a")
            for link in links:
                _url = link.get("href")
                seen_url = self.match(url=_url)
                if seen_url:
                    patch_urls.append(seen_url)
                else:
                    other_urls.append(_url)
        logger_str = get_spider_looger_str(
            "Debian", self.pkg, self.versions, self.cve, patch_urls, other_urls
        )
        logger.info(logger_str)
        for url in other_urls:
            text = "Uncertain patch information in Debian：%s" % url
            yield SavePipe(crawl=self, text=text)
        for patch_url in patch_urls:
            yield RequestsUrl(url=patch_url, crawl=self, is_patch=True)
            text = "find patch information in Debian：%s" % patch_url
            yield SavePipe(crawl=self, text=text)
        if not any([other_urls, patch_urls]):
            text = "Debian result:There is no patch information and it has not been repaired"
            yield SavePipe(crawl=self, text=text)


class Ubuntu(Crawl):
    """
    Get the PR address in the Ubuntu website
    """

    def __init__(self, cve, versions, pkg, out_path=DEFAULT_SAVE_PATH):
        """
        Initialize attribute
        Args:
            cve: cve number
        """
        super(Ubuntu, self).__init__(
            cve=cve, pkg=pkg, versions=versions, out_path=out_path
        )
        self.start_url = "https://ubuntu.com/security/" + self.cve

    def parse(self, response, **kwargs):
        """
            Parse the webpage and extract the url
        Args:
            response:webpage
        Returns:

        """

        git_list, other_list = [], []
        try:
            soup = BeautifulSoup(response.text, "lxml")
            content = soup.find(colspan="2")
            for url in content.find_all("a"):
                patch_url = url.get("href")
                seen_url = self.match(url=patch_url)
                if seen_url:
                    git_list.append(seen_url)
                else:
                    other_list.append(patch_url)

            logger_str = get_spider_looger_str(
                "Ubuntu", self.pkg, self.versions, self.cve, git_list, other_list
            )
            logger.info(logger_str)

            for patch in git_list:
                yield RequestsUrl(url=patch, crawl=self, is_patch=True)
                text = "find patch information in Ubuntu：%s" % patch
                yield SavePipe(crawl=self, text=text)
            for url in other_list:
                text = "Uncertain patch information in Ubuntu：%s" % url
                yield SavePipe(crawl=self, text=text)
        except:
            logger.info(
                "Ubuntu result:There is no patch information and it has not been repaired"
            )
            yield SavePipe(
                crawl=self,
                text="Ubuntu result:There is no patch information and it has not been repaired",
            )


class Bugzilla(Crawl):
    """
    Bug fix
    """

    bugzilla_api = Api(base_url="https://bugzilla.redhat.com")

    def __init__(self, cve, pkg, versions, out_path=DEFAULT_SAVE_PATH):
        self._v = Versions()
        super(Bugzilla, self).__init__(
            cve=cve, pkg=pkg, versions=versions, out_path=out_path
        )
        self.start_url = self.bugzilla_api.get_bug(fields=dict(alias=self.cve))

    @staticmethod
    def load_json(content):
        """
        Loading JSON data

        Args:
            content: The JSON content returned by the HTTP request
        """
        if not content:
            return dict()
        try:
            json_data = json.loads(content)
        except json.JSONDecodeError:
            json_data = dict()
        return json_data

    def _extract_fixed_in_version(self, bugs_info):
        """
        extract fixed in version

        Args:
            bugs_info: Description of CVE information
            pkg_name: The package name of the fix
        """
        fixed_in_versions = []
        for bug in bugs_info.get("bugs", []):
            fixed_in_versions.extend(
                self._v.match_version(self.pkg, bug.get("cf_fixed_in", ""))
            )

        return fixed_in_versions

    def _get_comments(self):
        """
        Get CVE comment information

        """
        status_code, response = self.bugzilla_api.get_comments(fields=self.cve)
        comments = dict()
        if status_code == requests.codes["ok"]:
            comments = Bugzilla.load_json(response)
        return comments

    def _filter_patch_remote(self, remote_url):
        """
        Filter out the patch pack path in GitHub based on the existing policy

        Args:
            remote_url:The remote address
        """
        if isinstance(remote_url, str):
            remote_url = [remote_url]
        patch_urls = list()

        for url in remote_url:
            seen_url = self.match(url=url)
            if seen_url:
                patch_urls.append(seen_url)

        return patch_urls

    def contrast_version(self, fix_versions, warehouse_versions):
        """
        Fix or affect version alignment
        """
        _fix_version = list()
        for _version in warehouse_versions:
            version_similarity = self._v.similarity(_version, fix_versions)
            compared_version = fix_versions[
                version_similarity.index(max(version_similarity))
            ]
            if self._v.lgt(_version, compared_version):
                _fix_version.append(_version)

        return _fix_version

    def _extract_url(self, comments):
        """
        Extracting comment information
        Args:
            comments:

        Returns:

        """
        pulls = []
        for _, comment in comments["bugs"].items():
            for comment_info in comment["comments"]:
                pulls.extend(re.findall(URL_REGEX, comment_info["text"]))
        return pulls

    def parse_comments(self, response, **kwargs):
        """
        Parse the bug comment content
        """
        comments = Bugzilla.load_json(response.text)
        if not comments:
            logger.info("Bugzilla result:{} has no comment".format(self.cve))
            yield SavePipe(
                crawl=self,
                text="Bugzilla result:There is no patch information and it has not been repaired",
            )

        patchs = self._filter_patch_remote(remote_url=self._extract_url(comments))
        if patchs:
            logger.info(f"Bugzilla result: found patch urls :{pprint.pformat(patchs)}")
            for patch in patchs:
                text = "find patch information in Bugzilla：%s" % patch
                yield SavePipe(crawl=self, text=text)
                yield RequestsUrl(url=patch, crawl=self, is_patch=True)
        else:
            logger.info("Bugzilla result:{} is not fixed yet".format(self.cve))
            yield SavePipe(
                crawl=self,
                text="Bugzilla result:no patch information in Bugzilla and it has not been fixed yet",
            )

    def parse(self, response, **kwargs):
        """
        Get the bug information for parsing
        """
        self.fix_versions = self._extract_fixed_in_version(
            bugs_info=Bugzilla.load_json(response.text)
        )

        yield RequestsUrl(
            url=self.bugzilla_api.get_comments(fields=self.cve),
            crawl=self,
            callback=self.parse_comments,
        )
